{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNスクラッチ\n",
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     15,
     24
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Z = 0\n",
    "        self.dA = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.Z = deepcopy(X)\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = deepcopy(dA)\n",
    "        dW = np.dot(self.Z.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T) \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     44,
     86
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Sigmoid」かTanhに向いている\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    ReLUと相性がいい\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = (self.sigma * np.random.randn(n_nodes1, n_nodes2))\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "code_folding": [
     25,
     51,
     83,
     98,
     121
    ]
   },
   "outputs": [],
   "source": [
    "class SGDrnn:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.WX[...] = layer.WX - self.lr * np.dot(layer.X.T, layer.dA) / len(layer.dA)\n",
    "        layer.B[...] = layer.B - self.lr * np.mean(layer.dA)\n",
    "        layer.Wh[...] = layer.Wh[...] - self.lr * np.dot(layer.ht.T, layer.dA) / len(layer.dA)\n",
    "        return layer\n",
    "    \n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W[...] = layer.W - self.lr * np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        layer.B[...] = layer.B - self.lr * np.mean(layer.dA, axis=0)\n",
    "        return layer\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    学習率を変化を減少させていく勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        self.HW += dW**2\n",
    "        self.HB +=  dB**2\n",
    "        layer.W[...] = layer.W - self.lr / np.sqrt(self.HW +1e-7) * dW #0で割るとまずいので +le-7\n",
    "        layer.B[...] = layer.B - self.lr / np.sqrt(self.HB + 1e-7)  * dB\n",
    "        return layer\n",
    "    \n",
    "class Momentum:\n",
    "    \n",
    "    \"\"\"\n",
    "    momentumSGD\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vW = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        self.vW = self.momentum * self.vW - self.lr * dW\n",
    "        self.vB =  self.momentum * self.vB - self.lr * dB\n",
    "        \n",
    "        layer.W[...] = layer.W + self.vW\n",
    "        layer.B[...] = layer.B + self.vB\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "class Adam:\n",
    "\n",
    "    \"\"\"\n",
    "    Adam\n",
    "    RMSprop に Momentum 法を組み合わせたような形\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    beta1\n",
    "    beta2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.mW = 0\n",
    "        self.vW = 0\n",
    "        self.mB = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \n",
    "        self.iter += 1\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter) \n",
    "        \n",
    "        self.mW += (1 - self.beta1) * (dW - self.mW)\n",
    "        self.vW += (1 - self.beta2) * (dW**2 - self.vW)\n",
    "        self.mB += (1 - self.beta1) * (dB - self.mB)\n",
    "        self.vB += (1 - self.beta2) * (dB**2 - self.vB)\n",
    "        \n",
    "        layer.W -= lr_t * self.mW / (np.sqrt(self.vW) + 1e-7)\n",
    "        layer.B -= lr_t * self.mB / (np.sqrt(self.vB) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     39,
     123
    ]
   },
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    \"\"\"\n",
    "    シグモイド関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = 1 / (1 + np.exp(-A))\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z) * self.Z \n",
    "        return dA\n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.tanh(A)\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z**2)\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        c = np.max(A)\n",
    "        A = A - c\n",
    "        ex = np.exp(A)\n",
    "        Z = ex / (np.sum(ex, axis=1))[:, np.newaxis]\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (batch_size, n_class)\n",
    "            正解ラベル\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_class)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = self.Z - y\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.maximum(0, A)\n",
    "        self.Z = deepcopy(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = dZ  *  np.where(self.Z != 0, 1, self.Z)\n",
    "        \n",
    "        return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    RNN 各シーケンスで出力するパターン\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, initializer, optimizer, activation):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.WX = initializer.W(n_features, n_nodes)\n",
    "        self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "        self.B = initializer.B(1)\n",
    "        self.n_nodes = n_nodes\n",
    "        self.A = None\n",
    "        self.ht = None\n",
    "        self.Z = None\n",
    "        self.dA = None\n",
    "        self.Xar = None\n",
    "        self.X = None\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_sequencesm n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        #self.Z = deepcopy(X)\n",
    "        self.Xar = X\n",
    "        m, s, n = X.shape\n",
    "        ht = np.zeros((m, self.n_nodes))\n",
    "        A = np.empty((0, m, self.n_nodes))\n",
    "        for i in range(s):\n",
    "            ht = np.dot(X[:, i, :], self.WX) + np.dot(ht, self.Wh) + self.B\n",
    "            ht = self.activation.forward(ht)\n",
    "            A = np.vstack((A, ht[np.newaxis,:])) #shape (シーケンス,　バッチ、n_node)\n",
    "            \n",
    "        A = A.transpose(1, 0, 2)\n",
    "        self.A = A\n",
    "        return A #shape (,バッチ、シーケンス、n_node)\n",
    "     \n",
    "    def backward(self, dA ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_sequences, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        #self.dA = deepcopy(dA)\n",
    "        m, s, n_nodes = self.A.shape\n",
    "        htd = 0\n",
    "        dX = np.zeros((s, m, self.n_features))\n",
    "        for i in reversed(range(s)):\n",
    "            da = dA[:, i,:]\n",
    "            da = da + htd\n",
    "            da = da  *  (1 - self.A[:, i, :]**2)#shape (m,n_nodes)\n",
    "            self.dA = da\n",
    "            self.X = self.Xar[:, i, :]\n",
    "            self.ht = self.A[:, i, :]\n",
    "            self = self.optimizer.update(self)\n",
    "            htd = np.dot(da, self.Wh.T) #shape(batch, n_nodes)\n",
    "            dX[i, :, :] = np.dot(da, self.WX.T) #dot後のshape (batch, n_features)\n",
    "            \n",
    "        dX = dX.transpose(1,0,2)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN2:\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    出力が最終層だけ\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, initializer, optimizer, activation):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.WX = initializer.W(n_features, n_nodes)\n",
    "        self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "        self.B = initializer.B(1)\n",
    "        self.n_nodes = n_nodes\n",
    "        self.A = None\n",
    "        self.ht = None\n",
    "        self.Z = None\n",
    "        self.dA = None\n",
    "        self.X = None\n",
    "        self.X_ar = None\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        ht : 次の形のndarray, shape (batch_size,n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X_ar = X\n",
    "        m, s, n = X.shape\n",
    "        ht = np.zeros((m, self.n_nodes))\n",
    "        A = np.empty((0, m, self.n_nodes))\n",
    "        for i in range(s):\n",
    "            ht = np.dot(X[:, i, :].reshape(m, n), self.WX) + np.dot(ht, self.Wh) + self.B\n",
    "            ht = self.activation.forward(ht)\n",
    "            A = np.vstack((A, ht[np.newaxis,:])) #shape (シーケンス,　バッチ、n_node)\n",
    "            \n",
    "        A = A.transpose(1, 0, 2)\n",
    "        self.A = A\n",
    "        return ht\n",
    "     \n",
    "    def backward(self, dA ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size,n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (batch_size, n_sequences, n_features)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        #self.dA = deepcopy(dA)\n",
    "        m, s, n_nodes = self.A.shape\n",
    "        htd = 0\n",
    "        dX = np.zeros((s, m, self.n_features))\n",
    "        for i in reversed(range(s)):\n",
    "            #da = da + htd\n",
    "            dA = dA * (1 - self.A[:, i, :]**2)#shape (m,n_nodes)\n",
    "            self.dA = dA\n",
    "            self.X = self.X_ar[:, i, :]\n",
    "            self.ht = self.A[:, i, :]\n",
    "            self = self.optimizer.update(self)\n",
    "            dA = np.dot(dA, self.Wh.T) #shape(batch, n_nodes)\n",
    "            dX[i, :, :] = np.dot(dA, self.WX.T) #dot後のshape (batch, n_features)\n",
    "            \n",
    "        dX = dX.transpose(1,0,2)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN2(2, 4, SimpleInitializer(), SGDrnn(lr=0.01), Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = rnn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここからライブドアニュースの分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブドアニュースでやりました\n",
    "from sklearn.datasets import load_files\n",
    "# encodingをutf-8指定して読み込み\n",
    "bin_data = load_files('../sprint22/text', encoding='utf-8')\n",
    "documents = bin_data.data\n",
    "# 今回はラベルが無いと仮定してください\n",
    "targets = bin_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning_list(lists):\n",
    "    clean_list = []\n",
    "    for text in lists:\n",
    "        BAD_SYMBOL = re.compile('^http.{1,100}\\n')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        BAD_SYMBOL = re.compile('^2.{1,40}\\n')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        BAD_SYMBOL = re.compile('[\\n*`\\s「」（）／]+')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        BAD_SYMBOL = re.compile('<.{1,40}>')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        BAD_SYMBOL = re.compile('【.{1,40}】')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        BAD_SYMBOL = re.compile(':.{1,10}:')\n",
    "        text = re.sub(BAD_SYMBOL, '', text)\n",
    "        \n",
    "        BAD_SYMBOL = re.compile('[0-9]+')\n",
    "        text = re.sub(BAD_SYMBOL, 'NUMBER', text)\n",
    "        \n",
    "        clean_list += [text]\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_documents = cleaning_list(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'あなたの打たれ強さ度は?イイ女を作る朝型生活など−週間ランキング人間関係をスムーズにするヒントやライフハック、節約ネタなど、今すぐ役立つ情報が詰まったPeachyのライフスタイルカテゴリ。このカテゴリのなかから、NUMBER年NUMBER月NUMBER日〜NUMBER月NUMBER日の間に最も多く読まれた記事TOPNUMBERをご紹介します！第NUMBER位：NUMBER年間洗顔せずお化粧を続けるとこうなる!?NUMBER日分のお化粧を施したらドロドロになってしもうたよあなたはこんなことを考えたことがありますか？私って、NUMBER年にどれくらい化粧品を使っているんだろう？そんな素朴な疑問を、モデルを使って実際に目に見える形にしてしまった、NUMBER人のオランダ人アーティストがいました。彼らの名は、Lernert&Sanderふたりとも男性です！。ただしこのふたりが表現したかったことは、人はNUMBER年に化粧品をどれくらい使うのかではなく、どれくらい化粧品を使えば、人は自然な状態からとんでもない状態になるのかということ。彼らはそうだ！化粧品をNUMBER年分塗り重ねていけば、きっととんでもなくなるはずだ！と、考えたわけです。第NUMBER位：あなたの打たれ強さ度を診断恋がうまくいかなかったり、理不尽なことで上司に怒られたり。生きていると凹むことっていっぱいありますよね。どん底気分のとき、あなたはすぐに回復できる人？それともなかなか立ち直れないほう？恋愛カウンセラー・ゆまさん監修の究極の恋愛科学では、あなたの打たれ強さ度をチェックできる心理テストを公開しています。第NUMBER位：レパートリーに悩むママ必見！夏のカンタン朝ごはんとは？あなたは毎日朝ごはんを食べていますか？何かと慌しい朝。時間がなくて毎日同じメニューなんて方も多いのではないでしょうか。ひとりだと適当にパンやおにぎりをコンビニで購入したり、ダイエットのために朝食を抜くこともあるかもしれませんが、家庭を持っているとそうはいきませんよね。第NUMBER位：いい女は朝を制す！朝型女子NUMBERの習慣おはようございますという挨拶からも気品が漂ってきそうな朝からシャキっと美しい女性、あなたの周りにいませんか？今回はそんな女性たちが実践しているであろう、いい女に欠かせない朝の習慣をまとめてみました。第NUMBER位：母性本能が強い女性NUMBERの特徴弱ってる男性を前にこの人は私がいないとダメだ、何とかしなきゃ！とつい使命感に燃えてしまうことありませんか？今回はそんな母性本能が強い女性の特徴をまとめてみました。以上、先週のライフスタイルカテゴリ週間ランキングでした！'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_documents[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "corpus = []\n",
    "for i in range(1000):\n",
    "   a = t.tokenize(c_documents[i], wakati=True)\n",
    "   corpus += [a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "model = FastText(corpus, size=100, min_count=1, window=4) #sg1がskip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335\n"
     ]
    }
   ],
   "source": [
    "mex = 0\n",
    "sample = 50\n",
    "for i in range(sample):\n",
    "    lenge = len(corpus[i])\n",
    "    if mex < lenge:\n",
    "        mex = lenge\n",
    "print(mex) #文章の最大のシーケン数\n",
    "s = mex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分散表現のアレーを作成\n",
    "#先頭シーケンス足りないところををゼロで埋める\n",
    "X_train = np.zeros((sample, s, 100))\n",
    "for i in range(sample):\n",
    "    textlists = corpus[i]\n",
    "    lenge = s - len(textlists)\n",
    "    for j, text in enumerate(textlists):\n",
    "        X_train[i, lenge+j, :] = model.wv[text].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = targets[:sample]\n",
    "y_train_hot = np.zeros((len(y_train), 9))\n",
    "for i in range(len(y_train)):\n",
    "    y_train_hot[i, y_train[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def _cross_entropy_loss(z, y):\n",
    "    z += 1e-7\n",
    "    return - sum(sum(y * np.log(z))) / len(y)\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    # accuracyを計算して返す\n",
    "    return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = SimpleRNN2(100,50,SimpleInitializer(), SGDrnn(lr=0.1), Tanh())\n",
    "fc = FC(50, 9,SimpleInitializer(), SGD(lr=0.1))\n",
    "sm = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :epoch\n",
      "loss\n",
      "2.198406688301769\n",
      "accuracy\n",
      "0.16\n",
      "2 :epoch\n",
      "loss\n",
      "2.1886991238404034\n",
      "accuracy\n",
      "0.22\n",
      "3 :epoch\n",
      "loss\n",
      "2.1789578911178538\n",
      "accuracy\n",
      "0.32\n",
      "4 :epoch\n",
      "loss\n",
      "2.168722747901421\n",
      "accuracy\n",
      "0.38\n",
      "5 :epoch\n",
      "loss\n",
      "2.1575702802930645\n",
      "accuracy\n",
      "0.46\n",
      "6 :epoch\n",
      "loss\n",
      "2.145107630862836\n",
      "accuracy\n",
      "0.52\n",
      "7 :epoch\n",
      "loss\n",
      "2.13097410372678\n",
      "accuracy\n",
      "0.54\n",
      "8 :epoch\n",
      "loss\n",
      "2.1148498150033226\n",
      "accuracy\n",
      "0.54\n",
      "9 :epoch\n",
      "loss\n",
      "2.096469729597557\n",
      "accuracy\n",
      "0.54\n",
      "10 :epoch\n",
      "loss\n",
      "2.075640364130962\n",
      "accuracy\n",
      "0.54\n",
      "11 :epoch\n",
      "loss\n",
      "2.0522555227487813\n",
      "accuracy\n",
      "0.54\n",
      "12 :epoch\n",
      "loss\n",
      "2.02630738902568\n",
      "accuracy\n",
      "0.54\n",
      "13 :epoch\n",
      "loss\n",
      "1.9978903845409608\n",
      "accuracy\n",
      "0.56\n",
      "14 :epoch\n",
      "loss\n",
      "1.9671969973714356\n",
      "accuracy\n",
      "0.6\n",
      "15 :epoch\n",
      "loss\n",
      "1.9345061348702848\n",
      "accuracy\n",
      "0.6\n",
      "16 :epoch\n",
      "loss\n",
      "1.9001650572757285\n",
      "accuracy\n",
      "0.6\n",
      "17 :epoch\n",
      "loss\n",
      "1.8645660908614894\n",
      "accuracy\n",
      "0.6\n",
      "18 :epoch\n",
      "loss\n",
      "1.8281200060791332\n",
      "accuracy\n",
      "0.64\n",
      "19 :epoch\n",
      "loss\n",
      "1.7912288823163989\n",
      "accuracy\n",
      "0.64\n",
      "20 :epoch\n",
      "loss\n",
      "1.7542619082110615\n",
      "accuracy\n",
      "0.66\n",
      "21 :epoch\n",
      "loss\n",
      "1.7175372742552701\n",
      "accuracy\n",
      "0.66\n",
      "22 :epoch\n",
      "loss\n",
      "1.68131237856545\n",
      "accuracy\n",
      "0.66\n",
      "23 :epoch\n",
      "loss\n",
      "1.6457828362431062\n",
      "accuracy\n",
      "0.66\n",
      "24 :epoch\n",
      "loss\n",
      "1.6110880393997606\n",
      "accuracy\n",
      "0.66\n",
      "25 :epoch\n",
      "loss\n",
      "1.5773188878259945\n",
      "accuracy\n",
      "0.66\n",
      "26 :epoch\n",
      "loss\n",
      "1.5445244632288593\n",
      "accuracy\n",
      "0.66\n",
      "27 :epoch\n",
      "loss\n",
      "1.512718127332322\n",
      "accuracy\n",
      "0.66\n",
      "28 :epoch\n",
      "loss\n",
      "1.4818846245317707\n",
      "accuracy\n",
      "0.66\n",
      "29 :epoch\n",
      "loss\n",
      "1.4519879119251635\n",
      "accuracy\n",
      "0.66\n",
      "30 :epoch\n",
      "loss\n",
      "1.4229785599781883\n",
      "accuracy\n",
      "0.66\n",
      "31 :epoch\n",
      "loss\n",
      "1.3948008877836018\n",
      "accuracy\n",
      "0.66\n",
      "32 :epoch\n",
      "loss\n",
      "1.3674015120588074\n",
      "accuracy\n",
      "0.66\n",
      "33 :epoch\n",
      "loss\n",
      "1.3407401026306416\n",
      "accuracy\n",
      "0.66\n",
      "34 :epoch\n",
      "loss\n",
      "1.3147954581224528\n",
      "accuracy\n",
      "0.66\n",
      "35 :epoch\n",
      "loss\n",
      "1.2895551228233353\n",
      "accuracy\n",
      "0.66\n",
      "36 :epoch\n",
      "loss\n",
      "1.2649982653581935\n",
      "accuracy\n",
      "0.68\n",
      "37 :epoch\n",
      "loss\n",
      "1.2410947804221235\n",
      "accuracy\n",
      "0.68\n",
      "38 :epoch\n",
      "loss\n",
      "1.2178153925363058\n",
      "accuracy\n",
      "0.68\n",
      "39 :epoch\n",
      "loss\n",
      "1.19513811365921\n",
      "accuracy\n",
      "0.68\n",
      "40 :epoch\n",
      "loss\n",
      "1.1730497133088063\n",
      "accuracy\n",
      "0.68\n",
      "41 :epoch\n",
      "loss\n",
      "1.1515448814395397\n",
      "accuracy\n",
      "0.68\n",
      "42 :epoch\n",
      "loss\n",
      "1.1306242079307023\n",
      "accuracy\n",
      "0.68\n",
      "43 :epoch\n",
      "loss\n",
      "1.1102913019819776\n",
      "accuracy\n",
      "0.7\n",
      "44 :epoch\n",
      "loss\n",
      "1.0905497241900963\n",
      "accuracy\n",
      "0.7\n",
      "45 :epoch\n",
      "loss\n",
      "1.0714006513234766\n",
      "accuracy\n",
      "0.7\n",
      "46 :epoch\n",
      "loss\n",
      "1.0528416706317079\n",
      "accuracy\n",
      "0.7\n",
      "47 :epoch\n",
      "loss\n",
      "1.0348665313566094\n",
      "accuracy\n",
      "0.7\n",
      "48 :epoch\n",
      "loss\n",
      "1.0174653655317318\n",
      "accuracy\n",
      "0.7\n",
      "49 :epoch\n",
      "loss\n",
      "1.0006250698078976\n",
      "accuracy\n",
      "0.7\n",
      "50 :epoch\n",
      "loss\n",
      "0.9843297103384152\n",
      "accuracy\n",
      "0.7\n",
      "51 :epoch\n",
      "loss\n",
      "0.968560990368039\n",
      "accuracy\n",
      "0.7\n",
      "52 :epoch\n",
      "loss\n",
      "0.9532987245515454\n",
      "accuracy\n",
      "0.72\n",
      "53 :epoch\n",
      "loss\n",
      "0.9385213672185897\n",
      "accuracy\n",
      "0.72\n",
      "54 :epoch\n",
      "loss\n",
      "0.9242065588456653\n",
      "accuracy\n",
      "0.72\n",
      "55 :epoch\n",
      "loss\n",
      "0.910331661942232\n",
      "accuracy\n",
      "0.74\n",
      "56 :epoch\n",
      "loss\n",
      "0.896874241413173\n",
      "accuracy\n",
      "0.74\n",
      "57 :epoch\n",
      "loss\n",
      "0.8838124279150452\n",
      "accuracy\n",
      "0.72\n",
      "58 :epoch\n",
      "loss\n",
      "0.8711251235641151\n",
      "accuracy\n",
      "0.72\n",
      "59 :epoch\n",
      "loss\n",
      "0.8587920202075305\n",
      "accuracy\n",
      "0.72\n",
      "60 :epoch\n",
      "loss\n",
      "0.8467934950624604\n",
      "accuracy\n",
      "0.74\n",
      "61 :epoch\n",
      "loss\n",
      "0.8351104605631834\n",
      "accuracy\n",
      "0.74\n",
      "62 :epoch\n",
      "loss\n",
      "0.8237242767097888\n",
      "accuracy\n",
      "0.74\n",
      "63 :epoch\n",
      "loss\n",
      "0.812616756048441\n",
      "accuracy\n",
      "0.76\n",
      "64 :epoch\n",
      "loss\n",
      "0.8017702157000332\n",
      "accuracy\n",
      "0.76\n",
      "65 :epoch\n",
      "loss\n",
      "0.7911675279966383\n",
      "accuracy\n",
      "0.8\n",
      "66 :epoch\n",
      "loss\n",
      "0.7807921171414967\n",
      "accuracy\n",
      "0.8\n",
      "67 :epoch\n",
      "loss\n",
      "0.7706279167152773\n",
      "accuracy\n",
      "0.82\n",
      "68 :epoch\n",
      "loss\n",
      "0.7606592779769527\n",
      "accuracy\n",
      "0.82\n",
      "69 :epoch\n",
      "loss\n",
      "0.7508709143694575\n",
      "accuracy\n",
      "0.8\n",
      "70 :epoch\n",
      "loss\n",
      "0.7412478986583956\n",
      "accuracy\n",
      "0.8\n",
      "71 :epoch\n",
      "loss\n",
      "0.7317758342476535\n",
      "accuracy\n",
      "0.8\n",
      "72 :epoch\n",
      "loss\n",
      "0.7224412936767629\n",
      "accuracy\n",
      "0.8\n",
      "73 :epoch\n",
      "loss\n",
      "0.7132326889372453\n",
      "accuracy\n",
      "0.82\n",
      "74 :epoch\n",
      "loss\n",
      "0.7041415849226541\n",
      "accuracy\n",
      "0.84\n",
      "75 :epoch\n",
      "loss\n",
      "0.6951642950090045\n",
      "accuracy\n",
      "0.84\n",
      "76 :epoch\n",
      "loss\n",
      "0.686302983438948\n",
      "accuracy\n",
      "0.84\n",
      "77 :epoch\n",
      "loss\n",
      "0.6775652031107513\n",
      "accuracy\n",
      "0.84\n",
      "78 :epoch\n",
      "loss\n",
      "0.6689612624717767\n",
      "accuracy\n",
      "0.84\n",
      "79 :epoch\n",
      "loss\n",
      "0.6605003669792786\n",
      "accuracy\n",
      "0.84\n",
      "80 :epoch\n",
      "loss\n",
      "0.652187635535158\n",
      "accuracy\n",
      "0.84\n",
      "81 :epoch\n",
      "loss\n",
      "0.6440233489450433\n",
      "accuracy\n",
      "0.84\n",
      "82 :epoch\n",
      "loss\n",
      "0.6360041302787891\n",
      "accuracy\n",
      "0.86\n",
      "83 :epoch\n",
      "loss\n",
      "0.6281246391375195\n",
      "accuracy\n",
      "0.86\n",
      "84 :epoch\n",
      "loss\n",
      "0.6203788162373144\n",
      "accuracy\n",
      "0.86\n",
      "85 :epoch\n",
      "loss\n",
      "0.6127604832436245\n",
      "accuracy\n",
      "0.86\n",
      "86 :epoch\n",
      "loss\n",
      "0.6052635544519888\n",
      "accuracy\n",
      "0.86\n",
      "87 :epoch\n",
      "loss\n",
      "0.5978820800073318\n",
      "accuracy\n",
      "0.86\n",
      "88 :epoch\n",
      "loss\n",
      "0.5906102615907206\n",
      "accuracy\n",
      "0.88\n",
      "89 :epoch\n",
      "loss\n",
      "0.5834424841707301\n",
      "accuracy\n",
      "0.88\n",
      "90 :epoch\n",
      "loss\n",
      "0.576373377912805\n",
      "accuracy\n",
      "0.88\n",
      "91 :epoch\n",
      "loss\n",
      "0.5693979038916904\n",
      "accuracy\n",
      "0.88\n",
      "92 :epoch\n",
      "loss\n",
      "0.5625114544205132\n",
      "accuracy\n",
      "0.9\n",
      "93 :epoch\n",
      "loss\n",
      "0.5557099657329179\n",
      "accuracy\n",
      "0.9\n",
      "94 :epoch\n",
      "loss\n",
      "0.548990022283651\n",
      "accuracy\n",
      "0.9\n",
      "95 :epoch\n",
      "loss\n",
      "0.5423489248946232\n",
      "accuracy\n",
      "0.9\n",
      "96 :epoch\n",
      "loss\n",
      "0.5357846931323814\n",
      "accuracy\n",
      "0.9\n",
      "97 :epoch\n",
      "loss\n",
      "0.5292959585158128\n",
      "accuracy\n",
      "0.9\n",
      "98 :epoch\n",
      "loss\n",
      "0.5228817875950638\n",
      "accuracy\n",
      "0.9\n",
      "99 :epoch\n",
      "loss\n",
      "0.5165414224896241\n",
      "accuracy\n",
      "0.9\n",
      "100 :epoch\n",
      "loss\n",
      "0.5102740660980668\n",
      "accuracy\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "#学習\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    #forward\n",
    "    X = rnn.forward(X_train)\n",
    "    X = fc.forward(X)\n",
    "    pred = sm.forward(X)\n",
    "    y_pred = np.argmax(pred, axis=1)\n",
    "    print(str(i+1) + \" :epoch\")\n",
    "    print(\"loss\")\n",
    "    print(_cross_entropy_loss(pred, y_train_hot))\n",
    "    print(\"accuracy\")\n",
    "    print(accuracy(y_train,y_pred))\n",
    "    #back\n",
    "    d = sm.backward(y_train_hot)\n",
    "    d = fc.backward(d)\n",
    "    d = rnn.backward(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
