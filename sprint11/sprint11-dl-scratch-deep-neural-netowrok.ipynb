{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディープラーニングをスクラッチする課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Z = 0\n",
    "        self.dA = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.Z = deepcopy(X)\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = deepcopy(dA)\n",
    "        dW = np.dot(self.Z.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T) \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】初期化方法のクラス化\n",
    "# 【問題6】重みの初期値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "code_folding": [
     0,
     44,
     86
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Sigmoid」かTanhに向いている\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    ReLUと相性がいい\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = (self.sigma * np.random.randn(n_nodes1, n_nodes2))\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】最適化手法のクラス化\n",
    " # 【問題7】最適化手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "code_folding": [
     0,
     25,
     57,
     95
    ]
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W[...] = layer.W - self.lr * np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        layer.B[...] = layer.B - self.lr * np.mean(layer.dA, axis=0)\n",
    "        return layer\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    学習率を変化を減少させていく勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        self.HW += dW**2\n",
    "        self.HB +=  dB**2\n",
    "        layer.W[...] = layer.W - self.lr / np.sqrt(self.HW +1e-7) * dW #0で割るとまずいので +le-7\n",
    "        layer.B[...] = layer.B - self.lr / np.sqrt(self.HB + 1e-7)  * dB\n",
    "        return layer\n",
    "    \n",
    "class Momentum:\n",
    "    \n",
    "    \"\"\"\n",
    "    momentumSGD\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vW = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        self.vW = self.momentum * self.vW - self.lr * dW\n",
    "        self.vB =  self.momentum * self.vB - self.lr * dB\n",
    "        \n",
    "        layer.W[...] = layer.W + self.vW\n",
    "        layer.B[...] = layer.B + self.vB\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "class Adam:\n",
    "\n",
    "    \"\"\"\n",
    "    Adam\n",
    "    RMSprop に Momentum 法を組み合わせたような形\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    beta1\n",
    "    beta2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.mW = 0\n",
    "        self.vW = 0\n",
    "        self.mB = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \n",
    "        self.iter += 1\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter) \n",
    "        \n",
    "        self.mW += (1 - self.beta1) * (dW - self.mW)\n",
    "        self.vW += (1 - self.beta2) * (dW**2 - self.vW)\n",
    "        self.mB += (1 - self.beta1) * (dB - self.mB)\n",
    "        self.vB += (1 - self.beta2) * (dB**2 - self.vB)\n",
    "        \n",
    "        layer.W -= lr_t * self.mW / (np.sqrt(self.vW) + 1e-7)\n",
    "        layer.B -= lr_t * self.mB / (np.sqrt(self.vB) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】活性化関数のクラス化\n",
    "# 【問題5】ReLUクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "code_folding": [
     0,
     39,
     78,
     123
    ]
   },
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    \"\"\"\n",
    "    シグモイド関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = 1 / (1 + np.exp(-A))\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z) * self.Z \n",
    "        return dA\n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.tanh(A)\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z**2)\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        c = np.max(A)\n",
    "        A = A - c\n",
    "        ex = np.exp(A)\n",
    "        Z = ex / (np.sum(ex, axis=1))[:, np.newaxis]\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (batch_size, n_class)\n",
    "            正解ラベル\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_class)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = self.Z - y\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.maximum(0, A)\n",
    "        self.Z = deepcopy(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = dZ  *  np.where(self.Z != 0, 1, self.Z)\n",
    "        \n",
    "        return dA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    #Pythonの特殊メソッドのひとつで、オブジェクトに角括弧でアクセスしたときの挙動を定義できる。\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "#これは使ってません\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, n_output, sigma=0.01, lr=0.01, batch_size=50, epoch=10, verbose=True, metrics=\"acc\"):\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_features = 0\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.metrics = metrics\n",
    "        self.verbose = verbose\n",
    "        self.train_loss = np.zeros(epoch)\n",
    "        self.val_loss = np.zeros(epoch)\n",
    "        self.layers = []\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        optimizer = SGD(self.lr)\n",
    "        #optimizer = AdaGrad(self.lr)\n",
    "        #Initializer = SimpleInitializer()\n",
    "        Initializer = HeInitializer()\n",
    "        \n",
    "        FC1 = FC(self.n_features, self.n_nodes1, Initializer, optimizer)\n",
    "        activation1 = ReLU()\n",
    "        FC2 = FC(self.n_nodes1, self.n_nodes2, Initializer, optimizer)\n",
    "        activation2 = ReLU()\n",
    "        FC3 = FC(self.n_nodes2, self.n_output, Initializer, optimizer)\n",
    "        activation3 = Softmax()\n",
    "        \n",
    "        self.construction(FC1)\n",
    "        self.construction(activation1)\n",
    "        self.construction(FC2)\n",
    "        self.construction(activation2)\n",
    "        self.construction(FC3)\n",
    "        self.construction(activation3)\n",
    "        \n",
    "        #self.construction(FC(self.n_features, self.n_nodes1, Initializer, optimizer))\n",
    "        #self.construction(ReLU())\n",
    "        #self.construction(b)\n",
    "        #self.construction(FC(self.n_nodes1, self.n_nodes2, Initializer, optimizer))\n",
    "        #self.construction(ReLU())\n",
    "        #self.construction(FC(self.n_nodes2, self.n_output, Initializer, optimizer))\n",
    "        #self.construction(Softmax())\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "\n",
    "            #バッチ作成\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=56)\n",
    "\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                #FP\n",
    "                self.FP(mini_X_train)\n",
    "\n",
    "                #BP\n",
    "                self.BP(mini_y_train)\n",
    "                \n",
    "            #評価値の表示\n",
    "            train_pred = self.FP(X)\n",
    "            self.train_loss[i] = self._cross_entropy_loss(train_pred, y)\n",
    "            \n",
    "            if np.any(X_val):\n",
    "                val_pred = self.FP(X_val)\n",
    "                self.val_loss[i] = self._cross_entropy_loss(val_pred, y_val)\n",
    "                met = self.accuracy(np.argmax(y_val, axis=1), np.argmax(val_pred, axis=1))\n",
    "                      \n",
    "                if self.verbose:\n",
    "                    print(\"epoch:{0} train_loss: {1} val_loss: {2} {3}: {4}\".format(i+1, self.train_loss[i], self.val_loss[i], self.metrics, met))\n",
    "                    \n",
    "            else:\n",
    "                if self.verbose:\n",
    "                      print(\"epoch:{0} loss: {1}\".format(i+1, self.train_loss[i]))\n",
    "     \n",
    "    def construction(self, layer):\n",
    "        self.layers += [layer]\n",
    "        \n",
    "    def FP(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "            \n",
    "    def BP(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "            \n",
    "    def _cross_entropy_loss(self,z, y):\n",
    "        z += 0.00000001\n",
    "        return - sum(sum(y * np.log(z))) / len(y)\n",
    "    \n",
    "    def accuracy(self, y, y_pred):\n",
    "        # accuracyを計算して返す\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, batch_size=50, epoch=10, verbose=True, metrics=\"acc\"):\n",
    "        self.n_nodes = [n_features]\n",
    "        self.n_output = n_output\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.metrics = metrics\n",
    "        self.verbose = verbose\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.layers = []\n",
    "\n",
    "    \n",
    "    def add(self, layer_type, n_nodes=None, Initializer=None, optimizer=None):\n",
    "        \n",
    "        if layer_type == \"FC\":\n",
    "            self.layers += [FC(self.n_nodes[-1], n_nodes, Initializer, optimizer)]\n",
    "            self.n_nodes += [n_nodes]\n",
    "            \n",
    "        elif layer_type == \"ReLU\":\n",
    "            self.layers += [ReLU()]\n",
    "        \n",
    "        elif layer_type == \"Tanh\":\n",
    "            self.layers += [Tanh()]\n",
    "        \n",
    "        elif layer_type == \"sigmoid\":\n",
    "            self.layers += [sigmoid()]\n",
    "            \n",
    "        elif layer_type == \"Softmax\":\n",
    "            self.layers += [Softmax()]\n",
    "        else:\n",
    "            print(\"layer_typeが存在しません\")\n",
    "            \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        epoch : int\n",
    "            エポック数変えたいときは入れてください\n",
    "        \"\"\"\n",
    "        if epoch:\n",
    "            self.epoch = epoch\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "\n",
    "            #バッチ作成\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=56)\n",
    "\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                #FP\n",
    "                self.FP(mini_X_train)\n",
    "\n",
    "                #BP\n",
    "                self.BP(mini_y_train)\n",
    "                \n",
    "            #評価値等の表示\n",
    "            train_pred = self.FP(X)\n",
    "            self.train_loss += [self._cross_entropy_loss(train_pred, y)]\n",
    "            \n",
    "            if np.any(X_val):\n",
    "                val_pred = self.FP(X_val)\n",
    "                self.val_loss += [self._cross_entropy_loss(val_pred, y_val)]\n",
    "                \n",
    "                #metricsを判定\n",
    "                if  self.metrics == \"acc\":\n",
    "                    met = self.accuracy(np.argmax(y_val, axis=1), np.argmax(val_pred, axis=1))\n",
    "                else:\n",
    "                    print(\"metricsの入力が間違っています\")\n",
    "                      \n",
    "                if self.verbose:\n",
    "                    print(\"epoch:{0} train_loss: {1} val_loss: {2} {3}: {4}\".format(i+1, self.train_loss[i], self.val_loss[i], self.metrics, met))\n",
    "                    \n",
    "            else:\n",
    "                if self.verbose:\n",
    "                      print(\"epoch:{0} loss: {1}\".format(i+1, self.train_loss[i]))\n",
    "     \n",
    "    def FP(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "            \n",
    "    def BP(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        hx = self.FP(X)\n",
    "        return np.argmax(hx, axis=1)\n",
    "            \n",
    "    def _cross_entropy_loss(self,z, y):\n",
    "        z += 1e-7\n",
    "        return - sum(sum(y * np.log(z))) / len(y)\n",
    "    \n",
    "    def accuracy(self, y, y_pred):\n",
    "        # accuracyを計算して返す\n",
    "        return accuracy_score(y, y_pred)\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"\n",
    "        学習曲線をプロットします。\n",
    "\n",
    "        loss : array\n",
    "        一回ごとの勾配降下方のロスのログ(train)\n",
    "         val_los : array\n",
    "        一回ごとの勾配降下方のロスのログ(val or test)\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.title(\"model_loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(self.train_loss, label=\"train_loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        #plt.yscale(\"log\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#データのロード\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#フラットにする\n",
    "X_train = x_train.reshape(-1, 784)\n",
    "X_test = x_test.reshape(-1, 784)\n",
    "#スケール合わせ\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "#onehot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "#sprit train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "model2 = ScratchDeepNeuralNetrowkClassifier2(n_features=n, batch_size=50, epoch=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = SGD(0.01)\n",
    "#optimizer1 = AdaGrad(0.01)\n",
    "#Initializer = SimpleInitializer()\n",
    "#Initializer = HeInitializer()\n",
    "#XavierInitializer\n",
    "#Momentum\n",
    "#Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(\"FC\", 400, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 400, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 400, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 200, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 200, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 100, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 100, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Tanh\")\n",
    "model2.add(\"FC\", 10, XavierInitializer(), Momentum(0.01, 0.9))\n",
    "model2.add(\"Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.FC at 0x1a38a2add8>,\n",
       " <__main__.Tanh at 0x1a38a2ad68>,\n",
       " <__main__.FC at 0x1a3d066e48>,\n",
       " <__main__.Tanh at 0x1a45dc32e8>,\n",
       " <__main__.FC at 0x1a3d066470>,\n",
       " <__main__.Tanh at 0x1a3d066ac8>,\n",
       " <__main__.FC at 0x1a3d0669b0>,\n",
       " <__main__.Tanh at 0x1a3d0665f8>,\n",
       " <__main__.FC at 0x1a3d066e80>,\n",
       " <__main__.Tanh at 0x1a3d0660b8>,\n",
       " <__main__.FC at 0x1a3d066f60>,\n",
       " <__main__.Tanh at 0x1a3d0666a0>,\n",
       " <__main__.FC at 0x1a3d066a20>,\n",
       " <__main__.Tanh at 0x1a3d066240>,\n",
       " <__main__.FC at 0x1a3d066f98>,\n",
       " <__main__.Softmax at 0x1a3d066ba8>]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss: 0.20406753222147625 val_loss: 0.22625635457227922 acc: 0.9301666666666667\n",
      "epoch:2 train_loss: 0.13403254699707032 val_loss: 0.16297755913294795 acc: 0.95225\n",
      "epoch:3 train_loss: 0.08944449551900228 val_loss: 0.12396296458292101 acc: 0.9640833333333333\n",
      "epoch:4 train_loss: 0.07124495728810629 val_loss: 0.11224698432382942 acc: 0.9673333333333334\n",
      "epoch:5 train_loss: 0.06567094167073567 val_loss: 0.11337616302820362 acc: 0.9681666666666666\n",
      "epoch:6 train_loss: 0.04723383665084839 val_loss: 0.09969378736409207 acc: 0.9716666666666667\n",
      "epoch:7 train_loss: 0.04317154471079508 val_loss: 0.10013958400554249 acc: 0.97175\n",
      "epoch:8 train_loss: 0.04041378633181254 val_loss: 0.10251083751366463 acc: 0.9718333333333333\n",
      "epoch:9 train_loss: 0.03125425553321838 val_loss: 0.10056137846035078 acc: 0.9734166666666667\n",
      "epoch:10 train_loss: 0.023156980593999225 val_loss: 0.09465511646492282 acc: 0.9759166666666667\n",
      "epoch:11 train_loss: 0.017184126496315004 val_loss: 0.09519316954745492 acc: 0.9756666666666667\n",
      "epoch:12 train_loss: 0.012797170042991638 val_loss: 0.09566807078106973 acc: 0.976\n",
      "epoch:13 train_loss: 0.019271175583203633 val_loss: 0.10094088014284897 acc: 0.9740833333333333\n",
      "epoch:14 train_loss: 0.01351869793732961 val_loss: 0.09863283297663376 acc: 0.9764166666666667\n",
      "epoch:15 train_loss: 0.009786605924367905 val_loss: 0.10087363764903617 acc: 0.9761666666666666\n",
      "epoch:16 train_loss: 0.010557283083597819 val_loss: 0.1070235496806284 acc: 0.97525\n",
      "epoch:17 train_loss: 0.00767886479695638 val_loss: 0.10176793412053614 acc: 0.9766666666666667\n",
      "epoch:18 train_loss: 0.010390363017717997 val_loss: 0.10833723951787395 acc: 0.9759166666666667\n",
      "epoch:19 train_loss: 0.004827556312084198 val_loss: 0.10243857102648261 acc: 0.9764166666666667\n",
      "epoch:20 train_loss: 0.003201223502556483 val_loss: 0.0960196456565972 acc: 0.9781666666666666\n",
      "epoch:21 train_loss: 0.004872741803526879 val_loss: 0.10713772091685962 acc: 0.9773333333333334\n",
      "epoch:22 train_loss: 0.0024396592353781066 val_loss: 0.09690790367733906 acc: 0.979\n",
      "epoch:23 train_loss: 0.0009631371075908343 val_loss: 0.09642787236529692 acc: 0.9796666666666667\n",
      "epoch:24 train_loss: 0.00040561407804489135 val_loss: 0.0937221825688083 acc: 0.9806666666666667\n",
      "epoch:25 train_loss: 0.0002962585985660553 val_loss: 0.09322909757186078 acc: 0.981\n",
      "epoch:26 train_loss: 0.0002462796395023664 val_loss: 0.09329138849716506 acc: 0.9805833333333334\n",
      "epoch:27 train_loss: 0.00021768471598625184 val_loss: 0.09346519219236529 acc: 0.9805\n",
      "epoch:28 train_loss: 0.00019675088052948316 val_loss: 0.09368360978817562 acc: 0.9805833333333334\n",
      "epoch:29 train_loss: 0.00018031189714868865 val_loss: 0.09390892019587545 acc: 0.98075\n",
      "epoch:30 train_loss: 0.00016687803094585736 val_loss: 0.09413186790949428 acc: 0.9808333333333333\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train.astype(\"f\"), y_train.astype(\"f\"), X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#勾配降下をMomentamで行った\n",
    "#収束が早い、　エポックが進んでもval_lossが上がっていかない\n",
    "model2.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ScratchDeepNeuralNetrowkClassifier2(n_features=n, batch_size=50, epoch=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.add(\"FC\", 400, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 400, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 400, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 200, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 200, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 100, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 100, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Tanh\")\n",
    "model3.add(\"FC\", 10, XavierInitializer(), Adam(lr=0.001, beta1=0.9, beta2=0.99))\n",
    "model3.add(\"Softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss: 0.20047947947184244 val_loss: 0.21675984774798368 acc: 0.10216666666666667\n",
      "epoch:2 train_loss: 0.147649112701416 val_loss: 0.1820328305480047 acc: 0.10216666666666667\n",
      "epoch:3 train_loss: 0.10643446159362793 val_loss: 0.15016149951632807 acc: 0.10216666666666667\n",
      "epoch:4 train_loss: 0.09567005157470704 val_loss: 0.1463943340791228 acc: 0.10216666666666667\n",
      "epoch:5 train_loss: 0.06966157658894857 val_loss: 0.12999290392276278 acc: 0.10216666666666667\n",
      "epoch:6 train_loss: 0.06545148118336995 val_loss: 0.1282480881990596 acc: 0.10216666666666667\n",
      "epoch:7 train_loss: 0.056506895542144775 val_loss: 0.1140373295413604 acc: 0.10216666666666667\n",
      "epoch:8 train_loss: 0.04453244137763977 val_loss: 0.12197214577978883 acc: 0.10216666666666667\n",
      "epoch:9 train_loss: 0.0545103751818339 val_loss: 0.12754291767693154 acc: 0.10216666666666667\n",
      "epoch:10 train_loss: 0.0439989321231842 val_loss: 0.12122807984219948 acc: 0.10216666666666667\n"
     ]
    }
   ],
   "source": [
    "model3.fit(X_train.astype(\"f\"), y_train.astype(\"f\"), X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#勾配降下をadamで行った\n",
    "#学習率高い(0.01)と収束しなかった。収束早い。beta2を下げると学習が進んだ時の学習率が低下？\n",
    "model3.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.101"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#テストのaccracy\n",
    "test_pred = model3.predict(X_test)\n",
    "model3.accuracy(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum SGD\n",
    "黒い線が損失関数の等高線を表していて、紫の星印で最小値となっているとします。Gradient Decent の更新式 (1.12) に従えば、傾きが最小の方向に突っ込んでいくためジグザグ運動が始まり、なかなか目的の星印にたどり着けなくなっています。しかし、よく観察してみると、行ったり来たりしている部分の\"平均\"をとればジグザグ運動は相殺されて前方への推進力（右図の青矢印）が得られそうです。これが、Momentum SGD の発想です。\n",
    "https://qiita.com/deaikei/items/29d4550fa5066184329a\n",
    "γは0.9 程度に設定されることが多いようです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
