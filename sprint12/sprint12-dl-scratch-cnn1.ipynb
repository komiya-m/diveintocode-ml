{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNをスクラッチする\n",
    "# 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Z = 0\n",
    "        self.dA = 0\n",
    "        self.dW = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.Z = deepcopy(X)\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dA = deepcopy(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA) / len(self.dA)\n",
    "        dZ = np.dot(dA, self.W.T) \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     48,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2=None):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        if n_nodes2 is None:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, 1)\n",
    "        else:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "            \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Sigmoid」かTanhに向いている\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = None\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")\n",
    "    \n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    ReLUと相性がいい\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 次の形のndarray, shape(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = (self.sigma * np.random.randn(n_nodes1, n_nodes2))\n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : 次の形のndarray, shape(1, nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     25,
     57,
     95
    ]
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W[...] = layer.W - self.lr * layer.dW\n",
    "        layer.B[...] = layer.B - self.lr * np.mean(layer.dA, axis=0, keepdims=True)\n",
    "        return layer\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    学習率を変化を減少させていく勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        self.HW += dW**2\n",
    "        self.HB +=  dB**2\n",
    "        layer.W[...] = layer.W - self.lr / np.sqrt(self.HW +1e-7) * dW #0で割るとまずいので +le-7\n",
    "        layer.B[...] = layer.B - self.lr / np.sqrt(self.HB + 1e-7)  * dB\n",
    "        return layer\n",
    "    \n",
    "class Momentum:\n",
    "    \n",
    "    \"\"\"\n",
    "    momentumSGD\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.vW = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        self.vW = self.momentum * self.vW - self.lr * dW\n",
    "        self.vB =  self.momentum * self.vB - self.lr * dB\n",
    "        \n",
    "        layer.W[...] = layer.W + self.vW\n",
    "        layer.B[...] = layer.B + self.vB\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "class Adam:\n",
    "\n",
    "    \"\"\"\n",
    "    Adam\n",
    "    RMSprop に Momentum 法を組み合わせたような形\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    momentum : 学習係数\n",
    "    beta1\n",
    "    beta2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.mW = 0\n",
    "        self.vW = 0\n",
    "        self.mB = 0\n",
    "        self.vB = 0\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \n",
    "        self.iter += 1\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter) \n",
    "        \n",
    "        self.mW += (1 - self.beta1) * (dW - self.mW)\n",
    "        self.vW += (1 - self.beta2) * (dW**2 - self.vW)\n",
    "        self.mB += (1 - self.beta1) * (dB - self.mB)\n",
    "        self.vB += (1 - self.beta2) * (dB**2 - self.vB)\n",
    "        \n",
    "        layer.W -= lr_t * self.mW / (np.sqrt(self.vW) + 1e-7)\n",
    "        layer.B -= lr_t * self.mB / (np.sqrt(self.vB) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     39,
     78,
     123
    ]
   },
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    \"\"\"\n",
    "    シグモイド関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = 1 / (1 + np.exp(-A))\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z) * self.Z \n",
    "        return dA\n",
    "    \n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.tanh(A)\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dA = dZ  *  (1 - self.Z**2)\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = 0\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        c = np.max(A)\n",
    "        A = A - c\n",
    "        ex = np.exp(A)\n",
    "        Z = ex / (np.sum(ex, axis=1))[:, np.newaxis]\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 次の形のndarray, shape (batch_size, n_class)\n",
    "            正解ラベル\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_class)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = self.Z - y\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        Z = np.maximum(0, A)\n",
    "        self.Z = deepcopy(Z)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dA = dZ  *  np.where(self.Z != 0, 1, self.Z)\n",
    "        \n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    #Pythonの特殊メソッドのひとつで、オブジェクトに角括弧でアクセスしたときの挙動を定義できる。\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, batch_size=50, epoch=10, verbose=True, metrics=\"acc\"):\n",
    "        self.n_nodes = [n_features]\n",
    "        #self.n_output = n_output\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.metrics = metrics\n",
    "        self.verbose = verbose\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.layers = []\n",
    "\n",
    "    \n",
    "    def add(self, layer_type, n_nodes=None, Initializer=None, optimizer=None):\n",
    "        \n",
    "        if layer_type == \"FC\":\n",
    "            self.layers += [FC(self.n_nodes[-1], n_nodes, Initializer, optimizer)]\n",
    "            self.n_nodes += [n_nodes]\n",
    "            \n",
    "        elif layer_type == \"Conv1d\":\n",
    "            salf.layers += [SimpleConv1d(filter_size=3, in_channel=1, initializer=initializer, optimizer=optimizer, straid=1, pad=0, out_channel=1)]\n",
    "            self.n_nodes += [n_nodes]\n",
    "            \n",
    "        elif layer_type == \"ReLU\":\n",
    "            self.layers += [ReLU()]\n",
    "        \n",
    "        elif layer_type == \"Tanh\":\n",
    "            self.layers += [Tanh()]\n",
    "        \n",
    "        elif layer_type == \"sigmoid\":\n",
    "            self.layers += [sigmoid()]\n",
    "            \n",
    "        elif layer_type == \"Softmax\":\n",
    "            self.layers += [Softmax()]\n",
    "        else:\n",
    "            print(\"layer_typeが存在しません\")\n",
    "    \n",
    "    def add_sim(self, layer):\n",
    "        \n",
    "        self.layers += [layer]\n",
    "            \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoch=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        epoch : int\n",
    "            エポック数変えたいときは入れてください\n",
    "        \"\"\"\n",
    "        if epoch:\n",
    "            self.epoch = epoch\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "\n",
    "            #バッチ作成\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=56)\n",
    "\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                #FP\n",
    "                self.FP(mini_X_train)\n",
    "\n",
    "                #BP\n",
    "                self.BP(mini_y_train)\n",
    "                \n",
    "            #評価値等の表示\n",
    "            train_pred = self.FP(X)\n",
    "            self.train_loss += [self._cross_entropy_loss(train_pred, y)]\n",
    "            \n",
    "            if np.any(X_val):\n",
    "                val_pred = self.FP(X_val)\n",
    "                self.val_loss += [self._cross_entropy_loss(val_pred, y_val)]\n",
    "                \n",
    "                #metricsを判定\n",
    "                if  self.metrics == \"acc\":\n",
    "                    met = self.accuracy(np.argmax(y_val, axis=1), np.argmax(val_pred, axis=1))\n",
    "                else:\n",
    "                    print(\"metricsの入力が間違っています\")\n",
    "                      \n",
    "                if self.verbose:\n",
    "                    print(\"epoch:{0} train_loss: {1} val_loss: {2} {3}: {4}\".format(i+1, self.train_loss[i], self.val_loss[i], self.metrics, met))\n",
    "                    \n",
    "            else:\n",
    "                if self.verbose:\n",
    "                      print(\"epoch:{0} loss: {1}\".format(i+1, self.train_loss[i]))\n",
    "     \n",
    "    def FP(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "            \n",
    "        return X\n",
    "            \n",
    "    def BP(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        hx = self.FP(X)\n",
    "        return np.argmax(hx, axis=1)\n",
    "            \n",
    "    def _cross_entropy_loss(self,z, y):\n",
    "        z += 1e-7\n",
    "        return - sum(sum(y * np.log(z))) / len(y)\n",
    "    \n",
    "    def accuracy(self, y, y_pred):\n",
    "        # accuracyを計算して返す\n",
    "        return accuracy_score(y, y_pred)\n",
    "    \n",
    "    def plot_learning_curve(self):\n",
    "        \"\"\"\n",
    "        学習曲線をプロットします。\n",
    "\n",
    "        loss : array\n",
    "        一回ごとの勾配降下方のロスのログ(train)\n",
    "         val_los : array\n",
    "        一回ごとの勾配降下方のロスのログ(val or test)\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.title(\"model_loss\")\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(self.train_loss, label=\"train_loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        #plt.yscale(\"log\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#古いやつ\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    convolutional layee\n",
    "    Parameters\n",
    "    ----------\n",
    "    filter_size : int\n",
    "      フィルターのサイズ\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    W : shape(output_channel)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size, in_channel, initializer, optimizer, straid=1, pad=0, out_channel=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_size = filter_size\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_size*in_channel, out_channel) #shape(filter_size*in_channel, out_cannel)\n",
    "        self.B = initializer.B(out_channel).reshape(1,-1) #shape(1, out_cannel)\n",
    "        self.out_size = None\n",
    "        self.out_channel = out_channel\n",
    "        self.straid = straid\n",
    "        self.CX = 0\n",
    "        self.dA = 0 #shape(batch_size, out_size)\n",
    "        self.dW = 0 \n",
    "        self.m = 0\n",
    "        self.n = 0\n",
    "        self.c = in_channel\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_feture, n_chanel)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, out_size, out_chanel)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        if X.ndim is 1:\n",
    "            X = X[np.newaxis, :, np.newaxis]\n",
    "        elif X.ndim is 2:\n",
    "            X = X[:, :, np.newaxis]\n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1]\n",
    "        self.c = X.shape[2]\n",
    "        #out_sizeの計算\n",
    "        if self.out_size is None:\n",
    "            self.out_size = int((self.n + 2*self.pad - self.filter_size) / self.straid + 1)\n",
    "        \n",
    "        #並列計算のためXからデータを抜き出す CX shape(out_size*batch_size, filter_size*in_chanel)\n",
    "        #CX = np.empty((0, self.filter_size*self.c))\n",
    "        #for i in range(self.m):\n",
    "            #CX = np.vstack((CX, np.array([X[i, j*self.straid : j*self.straid+self.filter_size, k] for j in range(self.out_size) for k in range(self.c)]).reshape(-1, self.filter_size*self.c)))\n",
    "        ar = np.array([np.arange(j * 1, j*1+3) for j in range(self.out_size)])\n",
    "        \n",
    "        CX = CX.astype(\"f\")\n",
    "        \n",
    "        self.CX = CX\n",
    "        \n",
    "        A = np.dot(CX, self.W) + self.B #ここのA shape(batch_size*out_size, out_cahnnel)\n",
    "        A = A.reshape(self.m, -1, self.out_channel) #ここのA shape(batch_size, out_size, out_cahnnel)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, out_size, out_channel)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_feture, in_channel)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        if dA.ndim is 1:\n",
    "            dA = dA[np.newaxis, :, np.newaxis]\n",
    "        elif dA.ndim is 2:\n",
    "            dA = dA[:, :, np.newaxis]\n",
    "            \n",
    "        self.dA = np.sum(dA, axis=1)\n",
    "        self.dW = np.dot(self.CX.T, dA.reshape(-1,self.out_channel)) #dAを shape(out_size*batch_size, out_channel)へ変形\n",
    "        \n",
    "        #ここからdZの計算\n",
    "        #dZの並列計算のためにdAのアレーを作成\n",
    "        da_ar = np.empty((0, self.filter_size*self.out_channel))\n",
    "        for i in range(self.m): #サンプル数分ループ\n",
    "            for j in range(self.n):\n",
    "                dalist = []\n",
    "                for k in range(self.out_channel):\n",
    "                    for s in range(self.filter_size):\n",
    "                        if  ((j - s) / self.straid < 0) or (j - s > self.out_size -1) or ((j - s) % self.straid  != 0):\n",
    "                            dalist += [0]\n",
    "                        else:\n",
    "                            dalist += [dA[i, j-s, k]]\n",
    "                dalist = np.array(dalist)\n",
    "                da_ar = np.vstack((da_ar, dalist)) #da_ar shape(batch_size*X_fetures, filter_size*out_channel)\n",
    "        \n",
    "        #Wのshapeを変形(filter_size*out_channel, in_channel)\n",
    "        W = self.W.T.reshape(self.c, self.out_channel, self.filter_size).transpose(1,0,2).reshape(self.c, -1).T\n",
    "        dZ = np.dot(da_ar, W).reshape(self.m, self.n, self.c)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def get_out_put_size(n_in, pad, filter_size, straid):\n",
    "        #一次元畳み込み後の出力サイズの計算\n",
    "        n_out = (n_in + 2*pad - filter_size) / straid + 1 \n",
    "        return n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    convolutional layee\n",
    "    Parameters\n",
    "    ----------\n",
    "    filter_size : int\n",
    "      フィルターのサイズ\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    W : shape(output_channel)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size, in_channel, initializer, optimizer, straid=1, pad=0, out_channel=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_size = filter_size\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(filter_size*in_channel, out_channel) #shape(filter_size*in_channel, out_cannel)\n",
    "        self.B = initializer.B(out_channel).reshape(1,-1) #shape(1, out_cannel)\n",
    "        self.out_size = None\n",
    "        self.out_channel = out_channel\n",
    "        self.straid = straid\n",
    "        self.CX = 0\n",
    "        self.dA = 0 #shape(batch_size, out_size)\n",
    "        self.dW = 0 \n",
    "        self.m = 0\n",
    "        self.n = 0\n",
    "        self.c = in_channel\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size,  n_feture, n_chanel)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, out_size, out_chanel)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        if X.ndim is 1:\n",
    "            X = X[np.newaxis, :, np.newaxis]\n",
    "        elif X.ndim is 2:\n",
    "            X = X[:, :, np.newaxis]\n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1]\n",
    "        self.c = X.shape[2]\n",
    "        #out_sizeの計算\n",
    "        if self.out_size is None:\n",
    "            self.out_size = int((self.n + 2*self.pad - self.filter_size) / self.straid + 1)\n",
    "        \n",
    "        #並列計算のためXからデータを抜き出す CX shape(out_size*batch_size, filter_size*in_chanel)\n",
    "        #CX = np.empty((0, self.filter_size*self.c))\n",
    "        #for i in range(self.m):\n",
    "            #CX = np.vstack((CX, np.array([X[i, j*self.straid : j*self.straid+self.filter_size, k] for j in range(self.out_size) for k in range(self.c)]).reshape(-1, self.filter_size*self.c)))\n",
    "        indar = np.array([np.arange(j * self.straid, j*self.straid+self.filter_size) for j in range(self.out_size)])\n",
    "        #CX = X[:, :, ar].reshape(self.m*self.out_size, self.filter_size) \n",
    "        CX = X[:, indar, : ].transpose(0,1,3,2).reshape(self.out_size*self.m, self.filter_size*self.c).astype(\"f\")\n",
    "        self.CX = CX #shape(self.out_size*self.m, self.filter_size*self.c)\n",
    "        \n",
    "        A = np.dot(CX, self.W) + self.B #ここのA shape(batch_size*out_size, out_cahnnel)\n",
    "        A = A.reshape(self.m, self.out_size, self.out_channel) #ここのA shape(batch_size, out_size, out_cahnnel)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size,out_size,out_channel)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size,n_feture, in_channel)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        if dA.ndim is 1:\n",
    "            dA = dA[np.newaxis, :, np.newaxis]\n",
    "        elif dA.ndim is 2:\n",
    "            dA = dA[:, :, np.newaxis]\n",
    "            \n",
    "        self.dA = np.sum(dA, axis=1)\n",
    "        self.dW = np.dot(self.CX.T, dA.reshape(-1,self.out_channel)) #dAを shape(out_size*batch_size, out_channel)へ変形\n",
    "        \n",
    "        #ここからdZの計算\n",
    "        #dZの並列計算のためにdAのアレーを作成\n",
    "        da_ar = np.empty((0, self.filter_size*self.out_channel))\n",
    "        for i in range(self.m): #サンプル数分ループ\n",
    "            for j in range(self.n):\n",
    "                dalist = []\n",
    "                for k in range(self.out_channel):\n",
    "                    for s in range(self.filter_size):\n",
    "                        if  ((j - s) / self.straid < 0) or (j - s > self.out_size -1) or ((j - s) % self.straid  != 0):\n",
    "                            dalist += [0]\n",
    "                        else:\n",
    "                            dalist += [dA[i, j-s, k]]\n",
    "        \n",
    "                dalist = np.array(dalist)\n",
    "                da_ar = np.vstack((da_ar, dalist)) #da_ar shape(batch_size*X_fetures, filter_size*out_channel)\n",
    "        #indar = np.array([np.arange(j * self.straid, j*self.straid+self.filter_size) for j in range(self.out_size)])\n",
    "        #Wのshapeを変形(filter_size*out_channel, in_channel)\n",
    "        #ar = np.array([np.arange(i*self.straid,i*self.straid+self.filter_size)[::-1] for i in range(self.out_size)])\n",
    "        #dA = dA[:, ar, :].transpose(0,1,3,2).reshape(-1,self.filter_size*self.out_channel)\n",
    "\n",
    "        W = self.W.T.reshape(self.c, self.out_channel, self.filter_size).transpose(1,0,2).reshape(self.c, -1).T\n",
    "        dZ = np.dot(da_ar, W).reshape(self.m, self.n, self.c)\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    def get_out_put_size(n_in, pad, filter_size, straid):\n",
    "        #一次元畳み込み後の出力サイズの計算\n",
    "        n_out = (n_in + 2*pad - filter_size) / straid + 1 \n",
    "        return n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flat:\n",
    "    #フラットにするだけのレイヤー\n",
    "    def __init__(self):\n",
    "        self.shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(self.shape[0], -1)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return dA.reshape(*self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データのロード\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#フラットにする\n",
    "X_train = x_train.reshape(-1, 784)\n",
    "X_test = x_test.reshape(-1, 784)\n",
    "#スケール合わせ\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "#onehot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "#sprit train and val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "# 【問題5】学習・推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_train.shape[1]\n",
    "model = ScratchDeepNeuralNetrowkClassifier2(n_features=n, batch_size=50, epoch=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_sim(SimpleConv1d(3, in_channel=1, initializer=SimpleInitializer(), optimizer=SGD(lr=0.01), straid=1, pad=0, out_channel=1))\n",
    "model.add_sim(ReLU())\n",
    "model.add_sim(SimpleConv1d(3, in_channel=1, initializer=SimpleInitializer(), optimizer=SGD(lr=0.01), straid=1, pad=0, out_channel=1))\n",
    "model.add_sim(ReLU())\n",
    "#model.add_sim(Tl/anh())\n",
    "model.add_sim(flat())\n",
    "model.add_sim(FC(780, 10, SimpleInitializer(), SGD(lr=0.01)))\n",
    "model.add_sim(ReLU())\n",
    "model.add_sim(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 train_loss: 2.3016451954841615 val_loss: 2.3018949898084005 acc: 0.13666666666666666\n",
      "epoch:2 train_loss: 2.3005625581741334 val_loss: 2.3008622749646506 acc: 0.16666666666666666\n",
      "epoch:3 train_loss: 2.298586924870809 val_loss: 2.2992156195640563 acc: 0.20333333333333334\n",
      "epoch:4 train_loss: 2.29389587243398 val_loss: 2.2953948108355204 acc: 0.20666666666666667\n",
      "epoch:5 train_loss: 2.2810595766703288 val_loss: 2.2836811780929565 acc: 0.24\n",
      "epoch:6 train_loss: 2.240877674818039 val_loss: 2.2457859937349953 acc: 0.2833333333333333\n",
      "epoch:7 train_loss: 2.118956369161606 val_loss: 2.1372778971989947 acc: 0.4033333333333333\n",
      "epoch:8 train_loss: 1.7955518386761347 val_loss: 1.8744383303324381 acc: 0.5566666666666666\n",
      "epoch:9 train_loss: 1.23298604875803 val_loss: 1.4132691720128059 acc: 0.6366666666666667\n",
      "epoch:10 train_loss: 0.7880950570106506 val_loss: 1.0490347622334957 acc: 0.7066666666666667\n",
      "epoch:11 train_loss: 0.5637632425408811 val_loss: 0.8764332331965367 acc: 0.71\n",
      "epoch:12 train_loss: 0.44250357323791834 val_loss: 0.7901377308291072 acc: 0.7233333333333334\n",
      "epoch:13 train_loss: 0.3648061935778242 val_loss: 0.7423494767149289 acc: 0.7366666666666667\n",
      "epoch:14 train_loss: 0.3089085130440071 val_loss: 0.7146619768502812 acc: 0.75\n",
      "epoch:15 train_loss: 0.26564404338161696 val_loss: 0.6981083745226109 acc: 0.7666666666666667\n",
      "epoch:16 train_loss: 0.2306072314234916 val_loss: 0.6878990225945988 acc: 0.7766666666666666\n",
      "epoch:17 train_loss: 0.20151606799581107 val_loss: 0.681786140324839 acc: 0.7766666666666666\n",
      "epoch:18 train_loss: 0.17728624154505573 val_loss: 0.6788861832375551 acc: 0.7766666666666666\n",
      "epoch:19 train_loss: 0.15712028044978069 val_loss: 0.6784885808224863 acc: 0.7866666666666666\n",
      "epoch:20 train_loss: 0.14029472451606126 val_loss: 0.6798577530625516 acc: 0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train[:300],y_train[:300], X_val[:300], y_val[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleConv1d.get_out_put_size(10,1,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d = SimpleConv1d(filter_size=3, in_channel=1, initializer=SimpleInitializer(), optimizer=SGD(lr=0.01), straid=1, pad=0, out_channel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.04673844],\n",
       "        [-0.06893237]]])"
      ]
     },
     "execution_count": 1094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_a = np.array([[10., 20.]]).reshape(1,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.06868526],\n",
       "        [-0.1375861 ],\n",
       "        [ 0.03402217],\n",
       "        [ 0.06890666]]])"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.,  60.],\n",
       "       [ 50., 100.],\n",
       "       [ 70., 140.]])"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29486826, -0.60166824],\n",
       "       [-0.50936663, -0.99595594],\n",
       "       [-0.6867786 , -1.405549  ]], dtype=float32)"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d.W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
